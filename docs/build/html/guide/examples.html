

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Examples &mdash; NEORL 1.1.3b documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme_overrides.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> NEORL
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                main (1.1.3b )
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">General Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Getting Started</a></li>
</ul>
<p class="caption"><span class="caption-text">Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules/a2c.html">Advantage Actor Critic (A2C)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/dqn.html">Deep Q Learning (DQN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/ppo2.html">Proximal Policy Optimisation (PPO)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/ga.html">Genetic Algorithms (GA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/es.html">Evolution Strategies (ES)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/pso.html">Particle Swarm Optimisation (PSO)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/de.html">Differential Evolution (DE)</a></li>
</ul>
<p class="caption"><span class="caption-text">Hyperparameter Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tune/grid.html">Grid Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tune/random.html">Random Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tune/evolu.html">Evolutionary Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tune/bayes.html">Bayesian Search</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples/ex1.html">Example 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/ex2.html">Example 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/ex3.html">Example 3</a></li>
</ul>
<p class="caption"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../misc/changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../misc/projects.html">Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../misc/contrib.html">Contributors</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NEORL</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Examples</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/guide/examples.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="examples">
<span id="id1"></span><h1>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h1>
<div class="section" id="try-it-online-with-colab-notebooks">
<h2>Try it online with Colab Notebooks!<a class="headerlink" href="#try-it-online-with-colab-notebooks" title="Permalink to this headline">¶</a></h2>
<p>All the following examples can be executed online using Google colab <img alt="colab" src="../_images/colab.svg" />
notebooks:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/araffin/rl-tutorial-jnrr19">Full Tutorial</a></p></li>
<li><p><a class="reference external" href="https://github.com/Stable-Baselines-Team/rl-colab-notebooks">All Notebooks</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/stable_baselines_getting_started.ipynb">Getting Started</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/saving_loading_dqn.ipynb">Training, Saving, Loading</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/multiprocessing_rl.ipynb">Multiprocessing</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/monitor_training.ipynb">Monitor Training and Plotting</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/atari_games.ipynb">Atari Games</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/breakout.ipynb">Breakout</a> (trained agent included)</p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/stable_baselines_her.ipynb">Hindsight Experience Replay</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/rl-baselines-zoo.ipynb">RL Baselines zoo</a></p></li>
</ul>
</div>
<div class="section" id="basic-usage-training-saving-loading">
<h2>Basic Usage: Training, Saving, Loading<a class="headerlink" href="#basic-usage-training-saving-loading" title="Permalink to this headline">¶</a></h2>
<p>In the following example, we will train, save and load a DQN model on the Lunar Lander environment.</p>
<a class="reference external image-reference" href="https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/saving_loading_dqn.ipynb"><img alt="../_images/try_it.png" src="../_images/try_it.png" style="width: 185.4px; height: 40.8px;" /></a>
<div class="figure align-default" id="id4">
<img alt="https://cdn-images-1.medium.com/max/960/1*f4VZPKOI0PYNWiwt0la0Rg.gif" src="https://cdn-images-1.medium.com/max/960/1*f4VZPKOI0PYNWiwt0la0Rg.gif" />
<p class="caption"><span class="caption-text">Lunar Lander Environment</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>LunarLander requires the python package <code class="docutils literal notranslate"><span class="pre">box2d</span></code>.
You can install it using <code class="docutils literal notranslate"><span class="pre">apt</span> <span class="pre">install</span> <span class="pre">swig</span></code> and then <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">box2d</span> <span class="pre">box2d-kengz</span></code></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">load</span></code> function re-creates model from scratch on each call, which can be slow.
If you need to e.g. evaluate same model with multiple different sets of parameters, consider
using <code class="docutils literal notranslate"><span class="pre">load_parameters</span></code> instead.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>

<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">DQN</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common.evaluation</span> <span class="kn">import</span> <span class="n">evaluate_policy</span>


<span class="c1"># Create environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;LunarLander-v2&#39;</span><span class="p">)</span>

<span class="c1"># Instantiate the agent</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="s1">&#39;MlpPolicy&#39;</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">prioritized_replay</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Train the agent</span>
<span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">2e5</span><span class="p">))</span>
<span class="c1"># Save the agent</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;dqn_lunar&quot;</span><span class="p">)</span>
<span class="k">del</span> <span class="n">model</span>  <span class="c1"># delete trained model to demonstrate loading</span>

<span class="c1"># Load the trained agent</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DQN</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;dqn_lunar&quot;</span><span class="p">)</span>

<span class="c1"># Evaluate the agent</span>
<span class="n">mean_reward</span><span class="p">,</span> <span class="n">std_reward</span> <span class="o">=</span> <span class="n">evaluate_policy</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">get_env</span><span class="p">(),</span> <span class="n">n_eval_episodes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Enjoy trained agent</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">action</span><span class="p">,</span> <span class="n">_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="n">obs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="multiprocessing-unleashing-the-power-of-vectorized-environments">
<h2>Multiprocessing: Unleashing the Power of Vectorized Environments<a class="headerlink" href="#multiprocessing-unleashing-the-power-of-vectorized-environments" title="Permalink to this headline">¶</a></h2>
<a class="reference external image-reference" href="https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/multiprocessing_rl.ipynb"><img alt="../_images/try_it.png" src="../_images/try_it.png" style="width: 185.4px; height: 40.8px;" /></a>
<div class="figure align-default" id="id5">
<img alt="https://cdn-images-1.medium.com/max/960/1*h4WTQNVIsvMXJTCpXm_TAw.gif" src="https://cdn-images-1.medium.com/max/960/1*h4WTQNVIsvMXJTCpXm_TAw.gif" />
<p class="caption"><span class="caption-text">CartPole Environment</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">stable_baselines.common.policies</span> <span class="kn">import</span> <span class="n">MlpPolicy</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common.vec_env</span> <span class="kn">import</span> <span class="n">SubprocVecEnv</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common</span> <span class="kn">import</span> <span class="n">set_global_seeds</span><span class="p">,</span> <span class="n">make_vec_env</span>
<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">ACKTR</span>

<span class="k">def</span> <span class="nf">make_env</span><span class="p">(</span><span class="n">env_id</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utility function for multiprocessed env.</span>

<span class="sd">    :param env_id: (str) the environment ID</span>
<span class="sd">    :param num_env: (int) the number of environments you wish to have in subprocesses</span>
<span class="sd">    :param seed: (int) the inital seed for RNG</span>
<span class="sd">    :param rank: (int) index of the subprocess</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">_init</span><span class="p">():</span>
        <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_id</span><span class="p">)</span>
        <span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span> <span class="o">+</span> <span class="n">rank</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">env</span>
    <span class="n">set_global_seeds</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_init</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">env_id</span> <span class="o">=</span> <span class="s2">&quot;CartPole-v1&quot;</span>
    <span class="n">num_cpu</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># Number of processes to use</span>
    <span class="c1"># Create the vectorized environment</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">SubprocVecEnv</span><span class="p">([</span><span class="n">make_env</span><span class="p">(</span><span class="n">env_id</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_cpu</span><span class="p">)])</span>

    <span class="c1"># Stable Baselines provides you with make_vec_env() helper</span>
    <span class="c1"># which does exactly the previous steps for you:</span>
    <span class="c1"># env = make_vec_env(env_id, n_envs=num_cpu, seed=0)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">ACKTR</span><span class="p">(</span><span class="n">MlpPolicy</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">25000</span><span class="p">)</span>

    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">action</span><span class="p">,</span> <span class="n">_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="using-callback-monitoring-training">
<h2>Using Callback: Monitoring Training<a class="headerlink" href="#using-callback-monitoring-training" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We recommend reading the <a class="reference external" href="callbacks.html">Callback section</a></p>
</div>
<p>You can define a custom callback function that will be called inside the agent.
This could be useful when you want to monitor training, for instance display live
learning curves in Tensorboard (or in Visdom) or save the best agent.
If your callback returns False, training is aborted early.</p>
<a class="reference external image-reference" href="https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/monitor_training.ipynb"><img alt="../_images/try_it.png" src="../_images/try_it.png" style="width: 185.4px; height: 40.8px;" /></a>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">DDPG</span>
<span class="kn">from</span> <span class="nn">stable_baselines.ddpg.policies</span> <span class="kn">import</span> <span class="n">LnMlpPolicy</span>
<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">results_plotter</span>
<span class="kn">from</span> <span class="nn">stable_baselines.bench</span> <span class="kn">import</span> <span class="n">Monitor</span>
<span class="kn">from</span> <span class="nn">stable_baselines.results_plotter</span> <span class="kn">import</span> <span class="n">load_results</span><span class="p">,</span> <span class="n">ts2xy</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common.noise</span> <span class="kn">import</span> <span class="n">AdaptiveParamNoiseSpec</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common.callbacks</span> <span class="kn">import</span> <span class="n">BaseCallback</span>


<span class="k">class</span> <span class="nc">SaveOnBestTrainingRewardCallback</span><span class="p">(</span><span class="n">BaseCallback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Callback for saving a model (the check is done every ``check_freq`` steps)</span>
<span class="sd">    based on the training reward (in practice, we recommend using ``EvalCallback``).</span>

<span class="sd">    :param check_freq: (int)</span>
<span class="sd">    :param log_dir: (str) Path to the folder where the model will be saved.</span>
<span class="sd">      It must contains the file created by the ``Monitor`` wrapper.</span>
<span class="sd">    :param verbose: (int)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">check_freq</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SaveOnBestTrainingRewardCallback</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_freq</span> <span class="o">=</span> <span class="n">check_freq</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dir</span> <span class="o">=</span> <span class="n">log_dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">log_dir</span><span class="p">,</span> <span class="s1">&#39;best_model&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_mean_reward</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">def</span> <span class="nf">_init_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Create folder if needed</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">save_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_on_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_calls</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>

          <span class="c1"># Retrieve training reward</span>
          <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ts2xy</span><span class="p">(</span><span class="n">load_results</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_dir</span><span class="p">),</span> <span class="s1">&#39;timesteps&#39;</span><span class="p">)</span>
          <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
              <span class="c1"># Mean training reward over the last 100 episodes</span>
              <span class="n">mean_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span>
              <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Num timesteps: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_timesteps</span><span class="p">))</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best mean reward: </span><span class="si">{:.2f}</span><span class="s2"> - Last mean reward per episode: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_mean_reward</span><span class="p">,</span> <span class="n">mean_reward</span><span class="p">))</span>

              <span class="c1"># New best model, you could save the agent here</span>
              <span class="k">if</span> <span class="n">mean_reward</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_mean_reward</span><span class="p">:</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">best_mean_reward</span> <span class="o">=</span> <span class="n">mean_reward</span>
                  <span class="c1"># Example for saving best model</span>
                  <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Saving new best model to </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">save_path</span><span class="p">))</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">save_path</span><span class="p">)</span>

        <span class="k">return</span> <span class="kc">True</span>

<span class="c1"># Create log dir</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="s2">&quot;tmp/&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Create and wrap the environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;LunarLanderContinuous-v2&#39;</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">Monitor</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">)</span>

<span class="c1"># Add some param noise for exploration</span>
<span class="n">param_noise</span> <span class="o">=</span> <span class="n">AdaptiveParamNoiseSpec</span><span class="p">(</span><span class="n">initial_stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">desired_action_stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># Because we use parameter noise, we should use a MlpPolicy with layer normalization</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DDPG</span><span class="p">(</span><span class="n">LnMlpPolicy</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">param_noise</span><span class="o">=</span><span class="n">param_noise</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Create the callback: check every 1000 steps</span>
<span class="n">callback</span> <span class="o">=</span> <span class="n">SaveOnBestTrainingRewardCallback</span><span class="p">(</span><span class="n">check_freq</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">)</span>
<span class="c1"># Train the agent</span>
<span class="n">time_steps</span> <span class="o">=</span> <span class="mf">1e5</span>
<span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">time_steps</span><span class="p">),</span> <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">)</span>

<span class="n">results_plotter</span><span class="o">.</span><span class="n">plot_results</span><span class="p">([</span><span class="n">log_dir</span><span class="p">],</span> <span class="n">time_steps</span><span class="p">,</span> <span class="n">results_plotter</span><span class="o">.</span><span class="n">X_TIMESTEPS</span><span class="p">,</span> <span class="s2">&quot;DDPG LunarLander&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h2>Atari Games<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="figure align-default" id="id6">
<img alt="../_images/breakout.gif" src="../_images/breakout.gif" />
<p class="caption"><span class="caption-text">Trained A2C agent on Breakout</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="id7">
<img alt="https://cdn-images-1.medium.com/max/960/1*UHYJE7lF8IDZS_U5SsAFUQ.gif" src="https://cdn-images-1.medium.com/max/960/1*UHYJE7lF8IDZS_U5SsAFUQ.gif" />
<p class="caption"><span class="caption-text">Pong Environment</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
<p>Training a RL agent on Atari games is straightforward thanks to <code class="docutils literal notranslate"><span class="pre">make_atari_env</span></code> helper function.
It will do <a class="reference external" href="https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/">all the preprocessing</a>
and multiprocessing for you.</p>
<a class="reference external image-reference" href="https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/atari_games.ipynb"><img alt="../_images/try_it.png" src="../_images/try_it.png" style="width: 185.4px; height: 40.8px;" /></a>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">stable_baselines.common.cmd_util</span> <span class="kn">import</span> <span class="n">make_atari_env</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common.vec_env</span> <span class="kn">import</span> <span class="n">VecFrameStack</span>
<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">ACER</span>

<span class="c1"># There already exists an environment generator</span>
<span class="c1"># that will make and wrap atari environments correctly.</span>
<span class="c1"># Here we are also multiprocessing training (num_env=4 =&gt; 4 processes)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_atari_env</span><span class="p">(</span><span class="s1">&#39;PongNoFrameskip-v4&#39;</span><span class="p">,</span> <span class="n">num_env</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Frame-stacking with 4 frames</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">VecFrameStack</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">n_stack</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ACER</span><span class="p">(</span><span class="s1">&#39;CnnPolicy&#39;</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">25000</span><span class="p">)</span>

<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">action</span><span class="p">,</span> <span class="n">_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="n">obs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="pybullet-normalizing-input-features">
<h2>PyBullet: Normalizing input features<a class="headerlink" href="#pybullet-normalizing-input-features" title="Permalink to this headline">¶</a></h2>
<p>Normalizing input features may be essential to successful training of an RL agent
(by default, images are scaled but not other types of input),
for instance when training on <a class="reference external" href="https://github.com/bulletphysics/bullet3/">PyBullet</a> environments. For that, a wrapper exists and
will compute a running average and standard deviation of input features (it can do the same for rewards).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>you need to install pybullet with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">pybullet</span></code></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">pybullet_envs</span>

<span class="kn">from</span> <span class="nn">stable_baselines.common.vec_env</span> <span class="kn">import</span> <span class="n">DummyVecEnv</span><span class="p">,</span> <span class="n">VecNormalize</span>
<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">PPO2</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">DummyVecEnv</span><span class="p">([</span><span class="k">lambda</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;HalfCheetahBulletEnv-v0&quot;</span><span class="p">)])</span>
<span class="c1"># Automatically normalize the input features and reward</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">VecNormalize</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">norm_obs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">norm_reward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                   <span class="n">clip_obs</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">PPO2</span><span class="p">(</span><span class="s1">&#39;MlpPolicy&#39;</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>

<span class="c1"># Don&#39;t forget to save the VecNormalize statistics when saving the agent</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="s2">&quot;/tmp/&quot;</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">log_dir</span> <span class="o">+</span> <span class="s2">&quot;ppo_halfcheetah&quot;</span><span class="p">)</span>
<span class="n">stats_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">log_dir</span><span class="p">,</span> <span class="s2">&quot;vec_normalize.pkl&quot;</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">stats_path</span><span class="p">)</span>

<span class="c1"># To demonstrate loading</span>
<span class="k">del</span> <span class="n">model</span><span class="p">,</span> <span class="n">env</span>

<span class="c1"># Load the agent</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">PPO2</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">log_dir</span> <span class="o">+</span> <span class="s2">&quot;ppo_halfcheetah&quot;</span><span class="p">)</span>

<span class="c1"># Load the saved statistics</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">DummyVecEnv</span><span class="p">([</span><span class="k">lambda</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;HalfCheetahBulletEnv-v0&quot;</span><span class="p">)])</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">VecNormalize</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">stats_path</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
<span class="c1">#  do not update them at test time</span>
<span class="n">env</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="kc">False</span>
<span class="c1"># reward normalization is not needed at test time</span>
<span class="n">env</span><span class="o">.</span><span class="n">norm_reward</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
<div class="section" id="custom-policy-network">
<h2>Custom Policy Network<a class="headerlink" href="#custom-policy-network" title="Permalink to this headline">¶</a></h2>
<p>Stable baselines provides default policy networks for images (CNNPolicies)
and other type of inputs (MlpPolicies).
However, you can also easily define a custom architecture for the policy network <a class="reference external" href="custom_policy.html">(see custom policy section)</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>

<span class="kn">from</span> <span class="nn">stable_baselines.common.policies</span> <span class="kn">import</span> <span class="n">FeedForwardPolicy</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common.vec_env</span> <span class="kn">import</span> <span class="n">DummyVecEnv</span>
<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">A2C</span>

<span class="c1"># Custom MLP policy of three layers of size 128 each</span>
<span class="k">class</span> <span class="nc">CustomPolicy</span><span class="p">(</span><span class="n">FeedForwardPolicy</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CustomPolicy</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
                                           <span class="n">net_arch</span><span class="o">=</span><span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="n">pi</span><span class="o">=</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span> <span class="n">vf</span><span class="o">=</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">])],</span>
                                           <span class="n">feature_extraction</span><span class="o">=</span><span class="s2">&quot;mlp&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">A2C</span><span class="p">(</span><span class="n">CustomPolicy</span><span class="p">,</span> <span class="s1">&#39;LunarLander-v2&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Train the agent</span>
<span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="accessing-and-modifying-model-parameters">
<h2>Accessing and modifying model parameters<a class="headerlink" href="#accessing-and-modifying-model-parameters" title="Permalink to this headline">¶</a></h2>
<p>You can access model’s parameters via <code class="docutils literal notranslate"><span class="pre">load_parameters</span></code> and <code class="docutils literal notranslate"><span class="pre">get_parameters</span></code> functions, which
use dictionaries that map variable names to NumPy arrays.</p>
<p>These functions are useful when you need to e.g. evaluate large set of models with same network structure,
visualize different layers of the network or modify parameters manually.</p>
<p>You can access original Tensorflow Variables with function <code class="docutils literal notranslate"><span class="pre">get_parameter_list</span></code>.</p>
<p>Following example demonstrates reading parameters, modifying some of them and loading them to model
by implementing <a class="reference external" href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/">evolution strategy</a>
for solving <code class="docutils literal notranslate"><span class="pre">CartPole-v1</span></code> environment. The initial guess for parameters is obtained by running
A2C policy gradient updates on the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">A2C</span>

<span class="k">def</span> <span class="nf">mutate</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Mutate parameters by adding normal noise to them&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">((</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return mean fitness (sum of episodic rewards) for given model&quot;&quot;&quot;</span>
    <span class="n">episode_rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">reward_sum</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span><span class="p">,</span> <span class="n">_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
            <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">reward_sum</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">episode_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_sum</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">)</span>

<span class="c1"># Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">)</span>
<span class="c1"># Create policy with a small network</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">A2C</span><span class="p">(</span><span class="s1">&#39;MlpPolicy&#39;</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">ent_coef</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
            <span class="n">policy_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;net_arch&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="p">]})</span>

<span class="c1"># Use traditional actor-critic policy gradient updates to</span>
<span class="c1"># find good initial parameters</span>
<span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

<span class="c1"># Get the parameters as the starting point for ES</span>
<span class="n">mean_params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span>

<span class="c1"># Include only variables with &quot;/pi/&quot; (policy) or &quot;/shared&quot; (shared layers)</span>
<span class="c1"># in their name: Only these ones affect the action.</span>
<span class="n">mean_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">mean_params</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                   <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;/pi/&quot;</span> <span class="ow">in</span> <span class="n">key</span> <span class="ow">or</span> <span class="s2">&quot;/shared&quot;</span> <span class="ow">in</span> <span class="n">key</span><span class="p">))</span>

<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># Create population of candidates and evaluate them</span>
    <span class="n">population</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">population_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">candidate</span> <span class="o">=</span> <span class="n">mutate</span><span class="p">(</span><span class="n">mean_params</span><span class="p">)</span>
        <span class="c1"># Load new policy parameters to agent.</span>
        <span class="c1"># Tell function that it should only update parameters</span>
        <span class="c1"># we give it (policy parameters)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_parameters</span><span class="p">(</span><span class="n">candidate</span><span class="p">,</span> <span class="n">exact_match</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">fitness</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
        <span class="n">population</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">candidate</span><span class="p">,</span> <span class="n">fitness</span><span class="p">))</span>
    <span class="c1"># Take top 10% and use average over their parameters as next mean parameter</span>
    <span class="n">top_candidates</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>
    <span class="n">mean_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">top_candidate</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">name</span><span class="p">]</span> <span class="k">for</span> <span class="n">top_candidate</span> <span class="ow">in</span> <span class="n">top_candidates</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">mean_params</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">mean_fitness</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">top_candidate</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">top_candidate</span> <span class="ow">in</span> <span class="n">top_candidates</span><span class="p">)</span> <span class="o">/</span> <span class="mf">10.0</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iteration </span><span class="si">{:&lt;3}</span><span class="s2"> Mean top fitness: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="n">mean_fitness</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="recurrent-policies">
<h2>Recurrent Policies<a class="headerlink" href="#recurrent-policies" title="Permalink to this headline">¶</a></h2>
<p>This example demonstrate how to train a recurrent policy and how to test it properly.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>One current limitation of recurrent policies is that you must test them
with the same number of environments they have been trained on.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">PPO2</span>

<span class="c1"># For recurrent policies, with PPO2, the number of environments run in parallel</span>
<span class="c1"># should be a multiple of nminibatches.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">PPO2</span><span class="p">(</span><span class="s1">&#39;MlpLstmPolicy&#39;</span><span class="p">,</span> <span class="s1">&#39;CartPole-v1&#39;</span><span class="p">,</span> <span class="n">nminibatches</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="mi">50000</span><span class="p">)</span>

<span class="c1"># Retrieve the env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_env</span><span class="p">()</span>

<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="c1"># Passing state=None to the predict function means</span>
<span class="c1"># it is the initial state</span>
<span class="n">state</span> <span class="o">=</span> <span class="kc">None</span>
<span class="c1"># When using VecEnv, done is a vector</span>
<span class="n">done</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">num_envs</span><span class="p">)]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="c1"># We need to pass the previous state and a mask for recurrent policies</span>
    <span class="c1"># to reset lstm state when a new episode begin</span>
    <span class="n">action</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">done</span><span class="p">)</span>
    <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span> <span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="c1"># Note: with VecEnv, env.reset() is automatically called</span>

    <span class="c1"># Show the env</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="hindsight-experience-replay-her">
<h2>Hindsight Experience Replay (HER)<a class="headerlink" href="#hindsight-experience-replay-her" title="Permalink to this headline">¶</a></h2>
<p>For this example, we are using <a class="reference external" href="https://github.com/eleurent/highway-env">Highway-Env</a> by <a class="reference external" href="https://github.com/eleurent">&#64;eleurent</a>.</p>
<a class="reference external image-reference" href="https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/master/stable_baselines_her.ipynb"><img alt="../_images/try_it.png" src="../_images/try_it.png" style="width: 185.4px; height: 40.8px;" /></a>
<div class="figure align-default" id="id8">
<img alt="https://raw.githubusercontent.com/eleurent/highway-env/gh-media/docs/media/parking-env.gif" src="https://raw.githubusercontent.com/eleurent/highway-env/gh-media/docs/media/parking-env.gif" />
<p class="caption"><span class="caption-text">The highway-parking-v0 environment.</span><a class="headerlink" href="#id8" title="Permalink to this image">¶</a></p>
</div>
<p>The parking env is a goal-conditioned continuous control task, in which the vehicle must park in a given space with the appropriate heading.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>the hyperparameters in the following example were optimized for that environment.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">highway_env</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">HER</span><span class="p">,</span> <span class="n">SAC</span><span class="p">,</span> <span class="n">DDPG</span><span class="p">,</span> <span class="n">TD3</span>
<span class="kn">from</span> <span class="nn">stable_baselines.ddpg</span> <span class="kn">import</span> <span class="n">NormalActionNoise</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;parking-v0&quot;</span><span class="p">)</span>

<span class="c1"># Create 4 artificial transitions per real transition</span>
<span class="n">n_sampled_goal</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># SAC hyperparams:</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HER</span><span class="p">(</span><span class="s1">&#39;MlpPolicy&#39;</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">SAC</span><span class="p">,</span> <span class="n">n_sampled_goal</span><span class="o">=</span><span class="n">n_sampled_goal</span><span class="p">,</span>
            <span class="n">goal_selection_strategy</span><span class="o">=</span><span class="s1">&#39;future&#39;</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">buffer_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">),</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
            <span class="n">policy_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]))</span>

<span class="c1"># DDPG Hyperparams:</span>
<span class="c1"># NOTE: it works even without action noise</span>
<span class="c1"># n_actions = env.action_space.shape[0]</span>
<span class="c1"># noise_std = 0.2</span>
<span class="c1"># action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions))</span>
<span class="c1"># model = HER(&#39;MlpPolicy&#39;, env, DDPG, n_sampled_goal=n_sampled_goal,</span>
<span class="c1">#             goal_selection_strategy=&#39;future&#39;,</span>
<span class="c1">#             verbose=1, buffer_size=int(1e6),</span>
<span class="c1">#             actor_lr=1e-3, critic_lr=1e-3, action_noise=action_noise,</span>
<span class="c1">#             gamma=0.95, batch_size=256,</span>
<span class="c1">#             policy_kwargs=dict(layers=[256, 256, 256]))</span>


<span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="mf">2e5</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;her_sac_highway&#39;</span><span class="p">)</span>

<span class="c1"># Load saved model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HER</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;her_sac_highway&#39;</span><span class="p">,</span> <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">)</span>

<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="c1"># Evaluate the agent</span>
<span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
  <span class="n">action</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
  <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
  <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
  <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span>
  <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;is_success&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reward:&quot;</span><span class="p">,</span> <span class="n">episode_reward</span><span class="p">,</span> <span class="s2">&quot;Success?&quot;</span><span class="p">,</span> <span class="n">info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;is_success&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>
    <span class="n">episode_reward</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="continual-learning">
<h2>Continual Learning<a class="headerlink" href="#continual-learning" title="Permalink to this headline">¶</a></h2>
<p>You can also move from learning on one environment to another for <a class="reference external" href="https://www.continualai.com/">continual learning</a>
(PPO2 on <code class="docutils literal notranslate"><span class="pre">DemonAttack-v0</span></code>, then transferred on <code class="docutils literal notranslate"><span class="pre">SpaceInvaders-v0</span></code>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">stable_baselines.common.cmd_util</span> <span class="kn">import</span> <span class="n">make_atari_env</span>
<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">PPO2</span>

<span class="c1"># There already exists an environment generator</span>
<span class="c1"># that will make and wrap atari environments correctly</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_atari_env</span><span class="p">(</span><span class="s1">&#39;DemonAttackNoFrameskip-v4&#39;</span><span class="p">,</span> <span class="n">num_env</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">PPO2</span><span class="p">(</span><span class="s1">&#39;CnnPolicy&#39;</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">action</span><span class="p">,</span> <span class="n">_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="n">obs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>

<span class="c1"># Close the processes</span>
<span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="c1"># The number of environments must be identical when changing environments</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_atari_env</span><span class="p">(</span><span class="s1">&#39;SpaceInvadersNoFrameskip-v4&#39;</span><span class="p">,</span> <span class="n">num_env</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># change env</span>
<span class="n">model</span><span class="o">.</span><span class="n">set_env</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">action</span><span class="p">,</span> <span class="n">_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="n">obs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
<span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="record-a-video">
<h2>Record a Video<a class="headerlink" href="#record-a-video" title="Permalink to this headline">¶</a></h2>
<p>Record a mp4 video (here using a random agent).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It requires ffmpeg or avconv to be installed on the machine.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common.vec_env</span> <span class="kn">import</span> <span class="n">VecVideoRecorder</span><span class="p">,</span> <span class="n">DummyVecEnv</span>

<span class="n">env_id</span> <span class="o">=</span> <span class="s1">&#39;CartPole-v1&#39;</span>
<span class="n">video_folder</span> <span class="o">=</span> <span class="s1">&#39;logs/videos/&#39;</span>
<span class="n">video_length</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">DummyVecEnv</span><span class="p">([</span><span class="k">lambda</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_id</span><span class="p">)])</span>

<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="c1"># Record the video starting at the first step</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">VecVideoRecorder</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">video_folder</span><span class="p">,</span>
                       <span class="n">record_video_trigger</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">video_length</span><span class="o">=</span><span class="n">video_length</span><span class="p">,</span>
                       <span class="n">name_prefix</span><span class="o">=</span><span class="s2">&quot;random-agent-</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">env_id</span><span class="p">))</span>

<span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">video_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
  <span class="n">action</span> <span class="o">=</span> <span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()]</span>
  <span class="n">obs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="c1"># Save the video</span>
<span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="bonus-make-a-gif-of-a-trained-agent">
<h2>Bonus: Make a GIF of a Trained Agent<a class="headerlink" href="#bonus-make-a-gif-of-a-trained-agent" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For Atari games, you need to use a screen recorder such as <a class="reference external" href="https://launchpad.net/kazam">Kazam</a>.
And then convert the video using <a class="reference external" href="https://superuser.com/questions/556029/how-do-i-convert-a-video-to-gif-using-ffmpeg-with-reasonable-quality">ffmpeg</a></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">imageio</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">A2C</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">A2C</span><span class="p">(</span><span class="s2">&quot;MlpPolicy&quot;</span><span class="p">,</span> <span class="s2">&quot;LunarLander-v2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="mi">100000</span><span class="p">)</span>

<span class="n">images</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;rgb_array&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">350</span><span class="p">):</span>
    <span class="n">images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">action</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="n">obs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;rgb_array&#39;</span><span class="p">)</span>

<span class="n">imageio</span><span class="o">.</span><span class="n">mimsave</span><span class="p">(</span><span class="s1">&#39;lander_a2c.gif&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">images</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">fps</span><span class="o">=</span><span class="mi">29</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Exelon Corp. &amp; MIT.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>